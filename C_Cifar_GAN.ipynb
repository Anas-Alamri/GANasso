{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1113,
     "status": "ok",
     "timestamp": 1556112357167,
     "user": {
      "displayName": "Sauli Sjögren",
      "photoUrl": "",
      "userId": "12974090036060883229"
     },
     "user_tz": -180
    },
    "id": "krZ_s2T8AyGP",
    "outputId": "f7d32294-e2bb-4b95-ee33-d27e47044bfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:  1312\n",
      "0.2.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from matplotlib.pyplot import imshow, imsave\n",
    "\n",
    "# Set random seem for reproducibility\n",
    "manualSeed = 1312\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1556112357172,
     "user": {
      "displayName": "Sauli Sjögren",
      "photoUrl": "",
      "userId": "12974090036060883229"
     },
     "user_tz": -180
    },
    "id": "gFpazwrKvrRU",
    "outputId": "51395031-7239-498d-c0a1-eba08c278321"
   },
   "outputs": [],
   "source": [
    "# This cell is only for Google Colaboratory\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "#\n",
    "path1 = \"train_images\" # Fake generated images here\n",
    "path2 = \"gen\"   # Generator parameters path\n",
    "path3 = \"dis\"   # Discriminator parameters path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5o86TewfDqHr"
   },
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"Art/Uusi_Kansio\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 100\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3 # with mnist 1, other 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "max_epoch = 100 # need more than 20 epochs for training generator\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0005\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Number of labels\n",
    "num_labels = 4 # we will use 4, CIFAR and MNIST has 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1843,
     "status": "ok",
     "timestamp": 1556112362115,
     "user": {
      "displayName": "Sauli Sjögren",
      "photoUrl": "",
      "userId": "12974090036060883229"
     },
     "user_tz": -180
    },
    "id": "aY896uDQAyG5",
    "outputId": "4039f4b0-d15f-4fba-de08-b4cafaaaf54e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Define which device you use for calculation. If you can use cuda, this will automatically use it. Otherwise cpu is automatically used.\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "98jC0W-gpnqQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "#transform=torchvision.transforms.Compose([\n",
    " #                                torchvision.transforms.Resize(image_size),\n",
    "  #                               torchvision.transforms.ToTensor(),\n",
    "   #                              torchvision.transforms.Normalize(\n",
    "    #                             (0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "#dataset =  torchvision.datasets.CIFAR10('/files/', train=True, download=True, transform = transform)\n",
    "\n",
    "#data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#real_batch = next(iter(data_loader))\n",
    "#plt.figure(figsize=(8,8))\n",
    "#plt.axis(\"off\")\n",
    "#plt.title(\"Training Images\")\n",
    "#plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(DEVICE)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9VWewv3ZvJz"
   },
   "outputs": [],
   "source": [
    "dataroot = []\n",
    "dataset = []\n",
    "# Create new dataset from images\n",
    "for i in range(2,num_labels + 1):\n",
    "    dataset.append(dset.ImageFolder(root=F\"./Dog_images/Label{i}\",\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])))\n",
    "\n",
    "# Create labels for our dataset\n",
    "# Labels are in i\n",
    "for i in range(len(dataset)):\n",
    "    for j in range(len(dataset[i])):\n",
    "        dataset[i].samples[j] = (dataset[i].samples[j][0],i)\n",
    "        \n",
    "# Combine all the datasets\n",
    "newdataset = torch.utils.data.ConcatDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88614,
     "status": "ok",
     "timestamp": 1556112456094,
     "user": {
      "displayName": "Sauli Sjögren",
      "photoUrl": "",
      "userId": "12974090036060883229"
     },
     "user_tz": -180
    },
    "id": "Da03iRyCZ27F",
    "outputId": "ddb878a9-722d-4b69-879b-fab3f97728b7"
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(newdataset, batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "#DEVICE = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch,labels = next(iter(dataloader))\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch.to(DEVICE)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "print(len(dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1b318tFHAyHT"
   },
   "outputs": [],
   "source": [
    "def normal_init(m, mean=0., std=0.02):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        #m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEbGlgA6AyHb"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional Discriminator for MNIST\n",
    "        MUUTTUJAT PÄIN VITTUA, KORJATKAA GLOBAALISTA LOKAAlIKSI\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel=1, input_size=784, condition_size=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(nc, ndf,4,2,1,bias=False)\n",
    "        \n",
    "        # first parameter is number of classes, condition_size is the same as number of classes\n",
    "        self.convlabel = nn.Conv2d(condition_size, ndf,4,2,1,bias=False)\n",
    "        self.conv2 = nn.Conv2d(ndf * 2,ndf * 4, 4, 2, 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv3 = nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv4 = nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(ndf * 16)\n",
    "        self.conv5 = nn.Conv2d(ndf * 16, 1, 4, 1, 0, bias=False)\n",
    "\n",
    "        self.act = nn.LeakyReLU(0.2,inplace=True)\n",
    "        self.out = nn.Sigmoid()\n",
    "        self.weight_init(mean = 0., std = 0.02)\n",
    "    \n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        \n",
    "        x = self.act(self.conv1(x))\n",
    "        y = self.act(self.convlabel(y))\n",
    "        x = torch.cat([x,y],1)\n",
    "        x = self.act(self.bn1(self.conv2(x)))\n",
    "        x = self.act(self.bn2(self.conv3(x)))\n",
    "        x = self.act(self.bn3(self.conv4(x)))\n",
    "        x = self.conv5(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmfBgT_zAyHl"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional Generator for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size = 100, condition_size=10):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(input_size, ngf * 8, 4, 1, 0,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(ngf*8)\n",
    "        \n",
    "        \n",
    "        # first parameter is number of classes\n",
    "        self.convlabel = nn.ConvTranspose2d(condition_size, ngf * 8, 4, 1, 0,bias=False)\n",
    "        self.bn1_1 = nn.BatchNorm2d(ngf * 8)\n",
    "        \n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "        self.conv2 = nn.ConvTranspose2d(ngf * 16,ngf * 8, 4, 2, 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(ngf * 8)\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(ngf * 8,ngf * 4, 4, 2, 1, bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(ngf * 4)\n",
    "        \n",
    "        self.conv4 = nn.ConvTranspose2d(ngf * 4,ngf * 2, 4, 2, 1, bias = False)\n",
    "        self.bn4 = nn.BatchNorm2d(ngf * 2)\n",
    "        \n",
    "        self.conv5 = nn.ConvTranspose2d(ngf*2, nc, 4, 2, 1, bias = False)\n",
    "        self.out = nn.Tanh()\n",
    "        self.weight_init(mean = 0., std = 0.02)\n",
    "    \n",
    "    def weight_init(self, mean, std):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        x = self.act(self.bn1(self.conv1(x)))\n",
    "        y = self.act(self.bn1_1(self.convlabel(y)))\n",
    "        x = torch.cat([x,y],1)\n",
    "        x = self.act(self.bn2(self.conv2(x)))\n",
    "        x = self.act(self.bn3(self.conv3(x)))\n",
    "        x = self.act(self.bn4(self.conv4(x)))\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 810,
     "status": "ok",
     "timestamp": 1556112466914,
     "user": {
      "displayName": "Sauli Sjögren",
      "photoUrl": "",
      "userId": "12974090036060883229"
     },
     "user_tz": -180
    },
    "id": "yv4vwx3YAyHy",
    "outputId": "3611d312-48af-4d84-e7ef-11f3c1751c67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "D = Discriminator(condition_size = num_labels).to(DEVICE)\n",
    "G = Generator(condition_size=num_labels).to(DEVICE)\n",
    "# D.load_state_dict('D_dc.pkl')\n",
    "# G.load_state_dict('G_dc.pkl')\n",
    "\n",
    "# Initialize discriminator and Generator weights\n",
    "#D.apply(weight_init)\n",
    "#G.apply(weight_init)\n",
    "\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "G_opt = torch.optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "print(ngf*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWe1eqkRAyI2"
   },
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn((batch_size, 100, 1, 1))\n",
    "fixed_noise = Variable(fixed_noise.to(DEVICE))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QXPmgEXAyJB"
   },
   "outputs": [],
   "source": [
    "# Initialize stuff before training\n",
    "step = 0\n",
    "n_critic = 1 # for training more k steps about Discriminator\n",
    "n_noise = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E4Xd-mMJAyJL"
   },
   "outputs": [],
   "source": [
    "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator Label to real\n",
    "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label to fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 98679,
     "status": "ok",
     "timestamp": 1556113363592,
     "user": {
      "displayName": "Sauli Sjögren",
      "photoUrl": "",
      "userId": "12974090036060883229"
     },
     "user_tz": -180
    },
    "id": "CApcMEraAyJV",
    "outputId": "64b90235-4054-43b9-d800-b1e3ad46a34f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/100][0]\tLoss_D: 1.5642\tLoss_G: 9.7523\tD(x): 0.5236\tD(G(z)): 0.5148 / 0.2897\n",
      "[0/100][20]\tLoss_D: 1.0975\tLoss_G: 4.2610\tD(x): 0.5906\tD(G(z)): 0.2039 / 0.0204\n",
      "[0/100][40]\tLoss_D: 0.1927\tLoss_G: 4.5532\tD(x): 0.8698\tD(G(z)): 0.0245 / 0.0153\n",
      "[0/100][60]\tLoss_D: 0.5853\tLoss_G: 2.8039\tD(x): 0.6785\tD(G(z)): 0.0048 / 0.1318\n",
      "[0/100][80]\tLoss_D: 0.7264\tLoss_G: 5.7354\tD(x): 0.8662\tD(G(z)): 0.4193 / 0.0045\n",
      "[0/100][100]\tLoss_D: 0.0701\tLoss_G: 3.7923\tD(x): 0.9659\tD(G(z)): 0.0329 / 0.0264\n",
      "[0/100][120]\tLoss_D: 1.1154\tLoss_G: 1.7570\tD(x): 0.4787\tD(G(z)): 0.1672 / 0.1879\n",
      "[0/100][140]\tLoss_D: 0.6046\tLoss_G: 4.5298\tD(x): 0.9260\tD(G(z)): 0.3932 / 0.0123\n",
      "[0/100][160]\tLoss_D: 1.1268\tLoss_G: 2.3879\tD(x): 0.4563\tD(G(z)): 0.1322 / 0.1156\n",
      "[0/100][180]\tLoss_D: 1.1494\tLoss_G: 3.0119\tD(x): 0.5830\tD(G(z)): 0.3048 / 0.0848\n",
      "[0/100][200]\tLoss_D: 0.4866\tLoss_G: 1.8297\tD(x): 0.6647\tD(G(z)): 0.0194 / 0.3486\n",
      "[0/100][220]\tLoss_D: 0.8433\tLoss_G: 3.9787\tD(x): 0.5463\tD(G(z)): 0.0084 / 0.0219\n",
      "[0/100][240]\tLoss_D: 0.8118\tLoss_G: 2.9856\tD(x): 0.7527\tD(G(z)): 0.3671 / 0.0703\n",
      "[0/100][260]\tLoss_D: 0.4402\tLoss_G: 1.4792\tD(x): 0.9025\tD(G(z)): 0.2745 / 0.2580\n",
      "[0/100][280]\tLoss_D: 1.0567\tLoss_G: 5.3590\tD(x): 0.8717\tD(G(z)): 0.5831 / 0.0057\n",
      "[0/100][300]\tLoss_D: 0.5993\tLoss_G: 2.7056\tD(x): 0.8648\tD(G(z)): 0.3425 / 0.0785\n",
      "[0/100][320]\tLoss_D: 0.9545\tLoss_G: 4.8084\tD(x): 0.6334\tD(G(z)): 0.3715 / 0.0087\n",
      "[0/100][340]\tLoss_D: 1.5505\tLoss_G: 3.7881\tD(x): 0.8117\tD(G(z)): 0.7330 / 0.0313\n",
      "[0/100][360]\tLoss_D: 1.3103\tLoss_G: 4.4442\tD(x): 0.6498\tD(G(z)): 0.3113 / 0.0129\n",
      "[0/100][380]\tLoss_D: 0.6725\tLoss_G: 4.5058\tD(x): 0.8161\tD(G(z)): 0.3639 / 0.0120\n",
      "[0/100][400]\tLoss_D: 0.3236\tLoss_G: 3.7134\tD(x): 0.7718\tD(G(z)): 0.0497 / 0.0324\n",
      "[0/100][420]\tLoss_D: 0.4057\tLoss_G: 0.3527\tD(x): 0.7675\tD(G(z)): 0.1208 / 0.7121\n",
      "[0/100][440]\tLoss_D: 0.5851\tLoss_G: 3.8383\tD(x): 0.5730\tD(G(z)): 0.0037 / 0.0246\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 81, in __getitem__\n    return self.datasets[dataset_idx][sample_idx]\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 101, in __getitem__\n    sample = self.loader(path)\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 147, in default_loader\n    return pil_loader(path)\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 130, in pil_loader\n    return img.convert('RGB')\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\PIL\\Image.py\", line 875, in convert\n    self.load()\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\PIL\\ImageFile.py\", line 233, in load\n    \"(%d bytes not processed)\" % len(b))\nOSError: image file is truncated (4054 bytes not processed)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-267e46d8569a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# Training Discriminator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[0mnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[1;31m# Python 2 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 81, in __getitem__\n    return self.datasets[dataset_idx][sample_idx]\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 101, in __getitem__\n    sample = self.loader(path)\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 147, in default_loader\n    return pil_loader(path)\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 130, in pil_loader\n    return img.convert('RGB')\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\PIL\\Image.py\", line 875, in convert\n    self.load()\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\PIL\\ImageFile.py\", line 233, in load\n    \"(%d bytes not processed)\" % len(b))\nOSError: image file is truncated (4054 bytes not processed)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Create arrays for Generator and Discriminator losses\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "img_list = []\n",
    "\n",
    "fixed_y = torch.zeros(num_labels, 1)\n",
    "y_label = torch.zeros(batch_size, num_labels)\n",
    "\n",
    "# Creates onehot array, that is diagonal matrix in tensor from.\n",
    "onehot = torch.zeros(num_labels, num_labels)\n",
    "onehot = onehot.scatter_(1, torch.LongTensor([range(num_labels)]).view(num_labels,1), 1).view(num_labels, num_labels, 1, 1)\n",
    "\n",
    "# Creates fill that makes image size layer with ones for each label\n",
    "fill = torch.zeros([num_labels, num_labels, image_size, image_size]).to(DEVICE)\n",
    "for i in range(num_labels):\n",
    "    fill[i, i, :, :] = 1\n",
    "\n",
    "\n",
    "\n",
    "# Fixed Y labels for evaluation\n",
    "y_0 = torch.full((batch_size//num_labels, 1), 0).type(torch.LongTensor).squeeze()\n",
    "y_fixed_labels = torch.zeros(batch_size, num_labels)\n",
    "\n",
    "for i in range(1,num_labels):\n",
    "    desired_label = i\n",
    "    y_fixed = torch.full((batch_size//num_labels, 1), desired_label).type(torch.LongTensor).squeeze()\n",
    "\n",
    "    y_0 = torch.cat((y_0, y_fixed), 0)\n",
    "\n",
    "y_fixed_labels = onehot[y_0]\n",
    "y_fixed_labels= Variable(y_fixed_labels.cuda())\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for idx, (images, labels) in enumerate(dataloader):\n",
    "\n",
    "        # Training Discriminator\n",
    "        D.zero_grad()\n",
    "        # Use print(labels.shape) to check how large batch size should be\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        \n",
    "        y_fill = fill[labels]\n",
    "        \n",
    "        x = images.to(DEVICE)\n",
    "        y = y_fill\n",
    "        x_outputs = D(x, y)\n",
    "        x_outputs = x_outputs.view(-1,1)\n",
    "        D_x = x_outputs.mean().item() # D(x|c) for real data\n",
    "        D_x_loss = criterion(x_outputs, D_labels)\n",
    "        \n",
    "        # Fake data:\n",
    "        \n",
    "        z = torch.randn((batch_size, 100, 1, 1))\n",
    "        y_rand = (torch.rand(batch_size, 1) * num_labels).type(torch.LongTensor).squeeze()\n",
    "        y_label = onehot[y_rand]\n",
    "        y_fill = fill[y_rand]\n",
    "        z = Variable(z.to(DEVICE))\n",
    "        y_label = Variable(y_label.to(DEVICE))\n",
    "        y_fill =  Variable(y_fill.to(DEVICE))\n",
    "        \n",
    "        fake_images = G(z, y_label)\n",
    "        \n",
    "        fake_outputs = D(fake_images, y_fill)\n",
    "        fake_outputs = fake_outputs.view(-1,1)\n",
    "        D_g_z1 = fake_outputs.mean().item() # D(G(z|c)|c) for fake data\n",
    "        D_z_loss = criterion(fake_outputs, D_fakes)\n",
    "         \n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "        \n",
    "        \n",
    "        D_loss.backward()\n",
    "        D_opt.step()\n",
    "        \n",
    "        if step % n_critic == 0:\n",
    "            # Training Generator\n",
    "            G.zero_grad()\n",
    "            z = torch.randn((batch_size, 100, 1, 1))\n",
    "            y_rand = (torch.rand(batch_size, 1) * num_labels).type(torch.LongTensor).squeeze()\n",
    "            y_label = onehot[y_rand]\n",
    "            y_fill = fill[y_rand]\n",
    "            z = Variable(z.to(DEVICE))\n",
    "            y_label = Variable(y_label.to(DEVICE))\n",
    "            y_fill =  Variable(y_fill.to(DEVICE))\n",
    "            \n",
    "            fake_images = G(z,y_label)\n",
    "            fake_outputs = D(fake_images, y_fill)\n",
    "            D_g_z2 = fake_outputs.mean().item() # D(G(z|c)|c) for fake data\n",
    "            fake_outputs = fake_outputs.view(-1,1)\n",
    "            G_loss = criterion(fake_outputs, D_labels)\n",
    "            \n",
    "            \n",
    "            G_loss.backward()\n",
    "            G_opt.step()       \n",
    "        if step % 20 == 0:\n",
    "            #print('Epoch: {}/{}, Step: {}, D Loss: {}, G Loss: {},  D(x): {}   D(G(z)): {} / {}'.format(epoch, max_epoch, \n",
    "            #                                        step, D_loss.item(), G_loss.item(),D_x,D_g_z1,D_g_z2))\n",
    "            print('[%d/%d][%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, max_epoch, step,\n",
    "                     D_loss.item(), G_loss.item(), D_x, D_g_z1, D_g_z2))\n",
    "            \n",
    "            # Output training stats\n",
    "        \n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(G_loss.item())\n",
    "            D_losses.append(D_loss.item())\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "    if (epoch % 1 == 0):\n",
    "        print(\"Figure number: \", epoch+1)\n",
    "        # generation to image\n",
    "        G.eval()\n",
    "\n",
    "        # Create labels from 0 to 9 (tensor is 100 long)\n",
    "       \n",
    "        # save image\n",
    "        \n",
    "        #torch.save(netG.state_dict(), os.path.join(path2, 'G--{}.ckpt'.format(epoch+1)))\n",
    "        #torch.save(netD.state_dict(), os.path.join(path3, 'D--{}.ckpt'.format(epoch+1)))\n",
    "        with torch.no_grad():\n",
    "            fake_images = G(fixed_noise,y_fixed_labels).detach().cpu()\n",
    "        \n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Training Images\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(fake_images.to(DEVICE)[:64], padding=5,pad_value = 1, normalize=True).cpu(),(1,2,0)))\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "        img_list.append(vutils.make_grid(fake_images, padding=2, normalize=True))\n",
    "        vutils.save_image((fake_images.data), os.path.join(path1, 'fake_images-{}.png'.format(epoch+1)), nrow = 10, normalize=True)\n",
    "        \n",
    "        G.train()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4757,
     "status": "ok",
     "timestamp": 1556113396626,
     "user": {
      "displayName": "Sauli Sjögren",
      "photoUrl": "",
      "userId": "12974090036060883229"
     },
     "user_tz": -180
    },
    "id": "ClRtye-1AyJh",
    "outputId": "0b42cc8d-af1d-4478-92d2-b9528832e624",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generation to image\n",
    "G.eval()\n",
    "\n",
    "# put here the desired label you want to be seen\n",
    "desired_label = 0\n",
    "\n",
    "y_rand = torch.full((batch_size, 1), desired_label).type(torch.LongTensor).squeeze()\n",
    "y_label = onehot[y_rand]\n",
    "#y_label = onehot.scatter_(1, torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).view(10,1), 1).view(10, 10, 1, 1)\n",
    "z = Variable(z.cuda())\n",
    "y_label = Variable(y_label.cuda())\n",
    "\n",
    "\n",
    "# Generate images using fixed noise and specific labels. For example \"generate fake landscape images\"\n",
    "with torch.no_grad():\n",
    "    fake_images = G(fixed_noise,y_label).detach().cpu()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Plot figures of specific labels    \n",
    "    \n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(fake_images.to(DEVICE)[:64], padding=5,pad_value = 1, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "\n",
    "# Plot each image separately.\n",
    "\n",
    "#print(fake_images[0][0].shape)\n",
    "#for i in range(20):\n",
    " #   plt.imshow(fake_images[i][0])\n",
    "  #  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9lDg_o9DAyJq"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, file_name='checkpoint.pth.tar'):\n",
    "    torch.save(state, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8iEGpiy8AyJz"
   },
   "outputs": [],
   "source": [
    "# Saving params.\n",
    "# torch.save(D.state_dict(), 'D_c.pkl')\n",
    "# torch.save(G.state_dict(), 'G_c.pkl')\n",
    "save_checkpoint({'epoch': epoch + 1, 'state_dict':D.state_dict(), 'optimizer' : D_opt.state_dict()}, 'D_dc.pth.tar')\n",
    "save_checkpoint({'epoch': epoch + 1, 'state_dict':G.state_dict(), 'optimizer' : G_opt.state_dict()}, 'G_dc.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjlyknuaBgdr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\matplotlib\\pyplot.py:537: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 81, in __getitem__\n    return self.datasets[dataset_idx][sample_idx]\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 101, in __getitem__\n    sample = self.loader(path)\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 147, in default_loader\n    return pil_loader(path)\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 130, in pil_loader\n    return img.convert('RGB')\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\PIL\\Image.py\", line 875, in convert\n    self.load()\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\PIL\\ImageFile.py\", line 233, in load\n    \"(%d bytes not processed)\" % len(b))\nOSError: image file is truncated (4054 bytes not processed)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0de3eea2eca4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m220\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_grid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpad_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    621\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 623\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 81, in __getitem__\n    return self.datasets[dataset_idx][sample_idx]\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 101, in __getitem__\n    sample = self.loader(path)\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 147, in default_loader\n    return pil_loader(path)\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 130, in pil_loader\n    return img.convert('RGB')\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\PIL\\Image.py\", line 875, in convert\n    self.load()\n  File \"C:\\Users\\Zaguero\\Anaconda3\\envs\\dle\\lib\\site-packages\\PIL\\ImageFile.py\", line 233, in load\n    \"(%d bytes not processed)\" % len(b))\nOSError: image file is truncated (4054 bytes not processed)\n"
     ]
    }
   ],
   "source": [
    "steps = 0\n",
    "for idx, (images, labels) in enumerate(dataloader):\n",
    "    if steps >= 220:\n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.imshow(np.transpose(vutils.make_grid(images.to(DEVICE)[:64], padding=5,pad_value = 1, normalize=True).cpu(),(1,2,0)))\n",
    "    if (steps % 10 ==0) :\n",
    "        print(steps)\n",
    "    steps += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AultrzIoBiEb"
   },
   "outputs": [],
   "source": [
    "print(1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C_Cifar_GAN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

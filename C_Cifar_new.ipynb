{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1556285172516,
     "user": {
      "displayName": "Mikael Helin",
      "photoUrl": "",
      "userId": "02507861142610635991"
     },
     "user_tz": -180
    },
    "id": "krZ_s2T8AyGP",
    "outputId": "e7d2947d-f736-427e-8c35-97e94e633508"
   },
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from matplotlib.pyplot import imshow, imsave\n",
    "\n",
    "# Set random seem for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 968,
     "status": "ok",
     "timestamp": 1556285172776,
     "user": {
      "displayName": "Mikael Helin",
      "photoUrl": "",
      "userId": "02507861142610635991"
     },
     "user_tz": -180
    },
    "id": "gFpazwrKvrRU",
    "outputId": "d1355244-db04-4236-a53e-1962bcfd57a2"
   },
   "outputs": [],
   "source": [
    "# This cell is only for Google Colaboratory\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')\n",
    "\n",
    "path1 = \"train_images\" # Fake generated images here\n",
    "path2 = \"gen\"   # Generator parameters path\n",
    "path3 = \"dis\"   # Discriminator parameters path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5o86TewfDqHr"
   },
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "dataroot = \"Art/Uusi_Kansio\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 2\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 100\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3 # with mnist 1, other 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Number of training epochs\n",
    "max_epoch = 800 # need more than 20 epochs for training generator\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Number of labels\n",
    "num_labels = 4 # we will use 4, CIFAR and MNIST has 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1556285172788,
     "user": {
      "displayName": "Mikael Helin",
      "photoUrl": "",
      "userId": "02507861142610635991"
     },
     "user_tz": -180
    },
    "id": "aY896uDQAyG5",
    "outputId": "f42b0ba4-c178-435a-eb74-a0d4d7b7f2b1"
   },
   "outputs": [],
   "source": [
    "# Define which device you use for calculation. If you can use cuda, this will automatically use it. Otherwise cpu is automatically used.\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2659,
     "status": "ok",
     "timestamp": 1556285176037,
     "user": {
      "displayName": "Mikael Helin",
      "photoUrl": "",
      "userId": "02507861142610635991"
     },
     "user_tz": -180
    },
    "id": "98jC0W-gpnqQ",
    "outputId": "df811efc-3cfd-4f30-e416-3982b8bba75d"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is a multiline\n",
    "comment.\n",
    "\n",
    "transform=torchvision.transforms.Compose([\n",
    "                                 torchvision.transforms.Resize(image_size),\n",
    "                                 torchvision.transforms.ToTensor(),\n",
    "                                 torchvision.transforms.Normalize(\n",
    "                                 (0.5,0.5,0.5), (0.5,0.5,0.5))])\n",
    "\n",
    "dataset =  torchvision.datasets.CIFAR10('/files/', train=True, download=True, transform = transform)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "real_batch = next(iter(data_loader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(DEVICE)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j9VWewv3ZvJz"
   },
   "outputs": [],
   "source": [
    "dataroot = []\n",
    "dataset = []\n",
    "# Create new dataset from images\n",
    "for i in range(1,num_labels + 1):\n",
    "    dataset.append(dset.ImageFolder(root=F\"./Dog_images/Label{i}\",\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.Resize(image_size),\n",
    "                               transforms.CenterCrop(image_size),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])))\n",
    "\n",
    "# Create labels for our dataset\n",
    "# Labels are in i\n",
    "print(len(dataset))\n",
    "print(len(dataset[0]))\n",
    "for i in range(len(dataset)):\n",
    "    for j in range(len(dataset[i])):\n",
    "        dataset[i].samples[j] = (dataset[i].samples[j][0],i)\n",
    "        \n",
    "# Combine all the datasets\n",
    "newdataset = torch.utils.data.ConcatDataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88614,
     "status": "ok",
     "timestamp": 1556112456094,
     "user": {
      "displayName": "Sauli SjÃ¶gren",
      "photoUrl": "",
      "userId": "12974090036060883229"
     },
     "user_tz": -180
    },
    "id": "Da03iRyCZ27F",
    "outputId": "ddb878a9-722d-4b69-879b-fab3f97728b7"
   },
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(newdataset, batch_size=batch_size,\n",
    "                                        shuffle=True, num_workers=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "#DEVICE = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch,labels = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch.to(DEVICE)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1b318tFHAyHT"
   },
   "outputs": [],
   "source": [
    "\n",
    "def normal_init(m, mean=0., std=0.5):\n",
    "    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        #m.bias.data.zero_()\n",
    "print(len(dataloader.dataset))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEbGlgA6AyHb"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional Discriminator for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel=1, input_size=784, condition_size=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(nc, ndf,4,2,1,bias=False)\n",
    "        \n",
    "        # first parameter is number of classes, condition_size is the same as number of classes\n",
    "        self.convlabel = nn.Conv2d(condition_size, ndf,4,2,1,bias=False)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(ndf * 2,ndf * 4, 4, 2, 1, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv3 = nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv4 = nn.Conv2d(ndf * 8, ndf * 16, 4, 2, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(ndf * 16)\n",
    "        self.conv5 = nn.Conv2d(ndf * 16, 1, 4, 1, 0, bias=False)\n",
    "\n",
    "        self.act = nn.LeakyReLU(0.2,inplace=True)\n",
    "        self.out = nn.Sigmoid()\n",
    "        #self.init_weights()\n",
    "        self.weight_init(mean=0.,std=0.02)\n",
    "        \n",
    "    def weight_init(self, mean=0., std=0.02):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "    \n",
    "                \n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        \n",
    "        x = self.act(self.conv1(x))\n",
    "        y = self.act(self.convlabel(y))\n",
    "        x = torch.cat([x,y],1)\n",
    "        x = self.act(self.bn1(self.conv2(x)))\n",
    "        x = self.act(self.bn2(self.conv3(x)))\n",
    "        x = self.act(self.bn3(self.conv4(x)))\n",
    "        x = self.conv5(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmfBgT_zAyHl"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional Generator for MNIST\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size = 100, condition_size=10):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(input_size, ngf * 8, 4, 1, 0,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(ngf*8)\n",
    "        \n",
    "        \n",
    "        # first parameter is number of classes\n",
    "        self.convlabel = nn.ConvTranspose2d(condition_size, ngf * 8, 4, 1, 0,bias=False)\n",
    "        self.bn1_1 = nn.BatchNorm2d(ngf * 8)\n",
    "        \n",
    "        self.act = nn.LeakyReLU(0.2,inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.ConvTranspose2d(ngf * 16,ngf * 8, 4, 2, 1, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(ngf * 8)\n",
    "\n",
    "        self.conv3 = nn.ConvTranspose2d(ngf * 8,ngf * 4, 4, 2, 1, bias = False)\n",
    "        self.bn3 = nn.BatchNorm2d(ngf * 4)\n",
    "        \n",
    "        self.conv4 = nn.ConvTranspose2d(ngf * 4,ngf * 2, 4, 2, 1, bias = False)\n",
    "        self.bn4 = nn.BatchNorm2d(ngf * 2)\n",
    "        \n",
    "        self.conv5 = nn.ConvTranspose2d(ngf*2, nc, 4, 2, 1, bias = False)\n",
    "        self.out = nn.Tanh()\n",
    "        self.weight_init()\n",
    "    \n",
    "    def weight_init(self, mean=0., std=0.02):\n",
    "        for m in self._modules:\n",
    "            normal_init(self._modules[m], mean, std)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        \n",
    "        x = self.act(self.bn1(self.conv1(x)))\n",
    "        y = self.act(self.bn1_1(self.convlabel(y)))\n",
    "        x = torch.cat([x,y],1)\n",
    "        x = self.act(self.bn2(self.conv2(x)))\n",
    "        x = self.act(self.bn3(self.conv3(x)))\n",
    "        x = self.act(self.bn4(self.conv4(x)))\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 897,
     "status": "ok",
     "timestamp": 1556285201084,
     "user": {
      "displayName": "Mikael Helin",
      "photoUrl": "",
      "userId": "02507861142610635991"
     },
     "user_tz": -180
    },
    "id": "yv4vwx3YAyHy",
    "outputId": "44fdb671-f4d1-47de-d81e-4686b7789dc4"
   },
   "outputs": [],
   "source": [
    "D = Discriminator(condition_size = num_labels).to(DEVICE)\n",
    "G = Generator(condition_size=num_labels).to(DEVICE)\n",
    "# D.load_state_dict('D_dc.pkl')\n",
    "# G.load_state_dict('G_dc.pkl')\n",
    "\n",
    "# Initialize discriminator and Generator weights\n",
    "#D.apply(weight_init)\n",
    "#G.apply(weight_init)\n",
    "\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "D_opt = torch.optim.Adam(D.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "G_opt = torch.optim.Adam(G.parameters(), lr=0.0005, betas=(0.5, 0.999))\n",
    "\n",
    "print(ngf*8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HWe1eqkRAyI2"
   },
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn((batch_size, 100, 1, 1))\n",
    "fixed_noise = Variable(fixed_noise.to(DEVICE))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QXPmgEXAyJB"
   },
   "outputs": [],
   "source": [
    "# Initialize stuff before training\n",
    "step = 0\n",
    "# n_critic should be between 2-6. With 3 results look promising, next testing 2 and 4.\n",
    "n_critic = 2 # for training more k steps about Discriminator.\n",
    "n_noise = 100\n",
    "#path1 = F\"/content/gdrive/My Drive/Colab Notebooks/Fakes\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1556285855498,
     "user": {
      "displayName": "Mikael Helin",
      "photoUrl": "",
      "userId": "02507861142610635991"
     },
     "user_tz": -180
    },
    "id": "E4Xd-mMJAyJL",
    "outputId": "31643f10-199f-408c-d59f-eed5c4d8e9b0"
   },
   "outputs": [],
   "source": [
    "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator Label to real\n",
    "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label to fake\n",
    "\n",
    "for i in range(len(D_labels)):\n",
    "    D_labels[i] = torch.add(D_labels[i],np.random.uniform(-0.1,0.1,1)[0])\n",
    "    D_fakes[i] = torch.add(D_fakes[i],np.random.uniform(0,0.2,1)[0])\n",
    "#print(D_labels.shape)\n",
    "#print(D_fakes.shape)\n",
    "#print(D_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 98679,
     "status": "ok",
     "timestamp": 1556113363592,
     "user": {
      "displayName": "Sauli SjÃ¶gren",
      "photoUrl": "",
      "userId": "12974090036060883229"
     },
     "user_tz": -180
    },
    "id": "CApcMEraAyJV",
    "outputId": "b4c75d32-f303-4e10-c6be-85a8a7c7cc63",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "#from PIL import ImageFile\n",
    "#ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Create arrays for Generator and Discriminator losses\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "img_list = []\n",
    "\n",
    "fixed_y = torch.zeros(num_labels, 1)\n",
    "y_label = torch.zeros(batch_size, num_labels)\n",
    "\n",
    "# Creates onehot array, that is diagonal matrix in tensor from.\n",
    "onehot = torch.zeros(num_labels, num_labels)\n",
    "onehot = onehot.scatter_(1, torch.LongTensor([range(num_labels)]).view(num_labels,1), 1).view(num_labels, num_labels, 1, 1)\n",
    "\n",
    "# Creates fill that makes image size layer with ones for each label\n",
    "fill = torch.zeros([num_labels, num_labels, image_size, image_size]).to(DEVICE)\n",
    "for i in range(num_labels):\n",
    "    fill[i, i, :, :] = 1\n",
    "\n",
    "\n",
    "\n",
    "# Fixed Y labels for evaluation\n",
    "y_0 = torch.full((batch_size//num_labels, 1), 0).type(torch.LongTensor).squeeze()\n",
    "y_fixed_labels = torch.zeros(batch_size, num_labels)\n",
    "\n",
    "for i in range(1,num_labels):\n",
    "    desired_label = i\n",
    "    y_fixed = torch.full((batch_size//num_labels, 1), desired_label).type(torch.LongTensor).squeeze()\n",
    "\n",
    "    y_0 = torch.cat((y_0, y_fixed), 0)\n",
    "\n",
    "y_fixed_labels = onehot[y_0]\n",
    "y_fixed_labels= Variable(y_fixed_labels.cuda())\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    for idx, (images, labels) in enumerate(dataloader):\n",
    "\n",
    "        # Training Discriminator\n",
    "        D.zero_grad()\n",
    "        # Use print(labels.shape) to check how large batch size should be\n",
    "        #print(labels.shape)\n",
    "        \n",
    "        \n",
    "        y_fill = fill[labels]\n",
    "        \n",
    "        x = images.to(DEVICE)\n",
    "        y = y_fill\n",
    "        x_outputs = D(x, y)\n",
    "        x_outputs = x_outputs.view(-1,1)\n",
    "        D_x = x_outputs.mean().item() # D(x|c) for real data\n",
    "        D_x_loss = criterion(x_outputs, D_labels)\n",
    "        \n",
    "        # Fake data:\n",
    "        \n",
    "        z = torch.randn((batch_size, 100, 1, 1))\n",
    "        y_rand = (torch.rand(batch_size, 1) * num_labels).type(torch.LongTensor).squeeze()\n",
    "        y_label = onehot[y_rand]\n",
    "        y_fill = fill[y_rand]\n",
    "        z = Variable(z.to(DEVICE))\n",
    "        y_label = Variable(y_label.to(DEVICE))\n",
    "        y_fill =  Variable(y_fill.to(DEVICE))\n",
    "        \n",
    "        fake_images = G(z, y_label)\n",
    "        \n",
    "        fake_outputs = D(fake_images, y_fill)\n",
    "        fake_outputs = fake_outputs.view(-1,1)\n",
    "        D_g_z1 = fake_outputs.mean().item() # D(G(z|c)|c) for fake data\n",
    "        D_z_loss = criterion(fake_outputs, D_fakes)\n",
    "         \n",
    "        D_loss = D_x_loss + D_z_loss\n",
    "        \n",
    "        \n",
    "        D_loss.backward()\n",
    "        D_opt.step()\n",
    "        \n",
    "        if step % n_critic == 0:\n",
    "            # Training Generator\n",
    "            G.zero_grad()\n",
    "            z = torch.randn((batch_size, 100, 1, 1))\n",
    "            y_rand = (torch.rand(batch_size, 1) * num_labels).type(torch.LongTensor).squeeze()\n",
    "            y_label = onehot[y_rand]\n",
    "            y_fill = fill[y_rand]\n",
    "            z = Variable(z.to(DEVICE))\n",
    "            y_label = Variable(y_label.to(DEVICE))\n",
    "            y_fill =  Variable(y_fill.to(DEVICE))\n",
    "            \n",
    "            fake_images = G(z,y_label)\n",
    "            \n",
    "            fake_outputs = D(fake_images, y_fill)\n",
    "            D_g_z2 = fake_outputs.mean().item() # D(G(z|c)|c) for fake data\n",
    "            fake_outputs = fake_outputs.view(-1,1)\n",
    "            G_loss = criterion(fake_outputs, D_labels)\n",
    "            \n",
    "            G_loss.backward()\n",
    "            G_opt.step()       \n",
    "        if step % 20 == 0:\n",
    "            #print('Epoch: {}/{}, Step: {}, D Loss: {}, G Loss: {}'.format(epoch, max_epoch, step, D_loss.item(), G_loss.item()))\n",
    "            # Output training stats\n",
    "            print('[%d/%d][%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, max_epoch, step,\n",
    "                     D_loss.item(), G_loss.item(), D_x, D_g_z1, D_g_z2))\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(G_loss.item())\n",
    "            D_losses.append(D_loss.item())\n",
    "            \n",
    "        step += 1\n",
    "        \n",
    "    if (epoch % 5 == 0):\n",
    "        print(\"Figure number: \", epoch+1)\n",
    "        # generation to image\n",
    "        G.eval()\n",
    "\n",
    "        # Create labels from 0 to 9 (tensor is 100 long)\n",
    "       \n",
    "        # save image\n",
    "        \n",
    "        #torch.save(netG.state_dict(), os.path.join(path2, 'G--{}.ckpt'.format(epoch+1)))\n",
    "        #torch.save(netD.state_dict(), os.path.join(path3, 'D--{}.ckpt'.format(epoch+1)))\n",
    "        with torch.no_grad():\n",
    "            fake_images = G(fixed_noise,y_fixed_labels).detach().cpu()\n",
    "        #vutils.make_grid(fake_images,padding=2,normalize = True)\n",
    "        plt.figure(figsize=(8,8))\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Training Images\")\n",
    "        plt.imshow(np.transpose(vutils.make_grid(fake_images.to(DEVICE), padding=5,pad_value = 1, nrow = 10, normalize=True).cpu(),(1,2,0)))\n",
    "        plt.show()  \n",
    "        \n",
    "        img_list.append(vutils.make_grid(fake_images, padding=2, nrow = 10, normalize=True))\n",
    "        vutils.save_image((fake_images.data), os.path.join(path1, 'fake_images-{}.png'.format(epoch+1)), nrow = 10, normalize=True)\n",
    "        \n",
    "        G.train()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 868,
     "status": "ok",
     "timestamp": 1556284038796,
     "user": {
      "displayName": "Mikael Helin",
      "photoUrl": "",
      "userId": "02507861142610635991"
     },
     "user_tz": -180
    },
    "id": "UiRNwfa8pq18",
    "outputId": "b2c4fb0d-394c-43eb-967b-0bc5e8138dc4"
   },
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "\n",
    "#Labels of Cifar-10 \n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u_krI8Wwpq2J"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ClRtye-1AyJh",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# generation to image\n",
    "G.eval()\n",
    "\n",
    "# put here the desired label you want to be seen\n",
    "desired_label = 0\n",
    "\n",
    "y_rand = torch.full((batch_size, 1), desired_label).type(torch.LongTensor).squeeze()\n",
    "y_label = onehot[y_rand]\n",
    "#y_label = onehot.scatter_(1, torch.LongTensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).view(10,1), 1).view(10, 10, 1, 1)\n",
    "z = Variable(z.cuda())\n",
    "y_label = Variable(y_label.cuda())\n",
    "\n",
    "\n",
    "# Generate images using fixed noise and specific labels. For example \"generate fake landscape images\"\n",
    "with torch.no_grad():\n",
    "    fake_images = G(fixed_noise,y_label).detach().cpu()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Plot figures of specific labels    \n",
    "    \n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(fake_images.to(DEVICE)[:64], padding=5,pad_value = 1, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "\n",
    "# Plot each image separately.\n",
    "\n",
    "#print(fake_images[0][0].shape)\n",
    "#for i in range(20):\n",
    " #   plt.imshow(fake_images[i][0])\n",
    "  #  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9lDg_o9DAyJq"
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, file_name='checkpoint.pth.tar'):\n",
    "    torch.save(state, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8iEGpiy8AyJz"
   },
   "outputs": [],
   "source": [
    "# Saving params.\n",
    "# torch.save(D.state_dict(), 'D_c.pkl')\n",
    "# torch.save(G.state_dict(), 'G_c.pkl')\n",
    "save_checkpoint({'epoch': epoch + 1, 'state_dict':D.state_dict(), 'optimizer' : D_opt.state_dict()}, 'D_dc.pth.tar')\n",
    "save_checkpoint({'epoch': epoch + 1, 'state_dict':G.state_dict(), 'optimizer' : G_opt.state_dict()}, 'G_dc.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BjlyknuaBgdr"
   },
   "outputs": [],
   "source": [
    "## 2 Kuvan interpolointi:\n",
    "# Fake_images on 100 kuvan batch, sieltÃ¤ pitÃ¤Ã¤ valita 2 kuvaa samasta classista\n",
    "# indeksit niiden mukaan\n",
    "\n",
    "flatted1 = fake_images[92].view(-1)\n",
    "flatted2 = fake_images[60].view(-1)\n",
    "\n",
    "interpolations = np.linspace(flatted1[:],flatted2[:],9)\n",
    "#print(interpolations.shape)\n",
    "plt.figure(1,figsize=(25,30))\n",
    "\n",
    "for i in range(interpolations.shape[0]):\n",
    "    plt.subplot(191+i) # plottauksen ja muun vois varmaan tehdÃ¤ jÃ¤rkevÃ¤mminkin...\n",
    "    fig1 = torch.tensor(interpolations[i,:])\n",
    "    fig1 = fig1.view(-1,64,64)\n",
    "    plt.imshow(np.transpose(vutils.make_grid(fig1.to(DEVICE), normalize=True).cpu(),(1,2,0)))\n",
    "    plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AultrzIoBiEb"
   },
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as scp\n",
    "\n",
    "Kuva_nmro = 92\n",
    "flatted = fake_images[Kuva_nmro].view(1,-1) # TÃ¤hÃ¤n kuva mille etitÃ¤Ã¤n vastaavanlaista\n",
    "\n",
    "N = 10000 # Kuvien mÃ¤Ã¤rÃ¤ siinÃ¤ classissa mistÃ¤ haetaan, pitÃ¤Ã¤ mÃ¤Ã¤ritellÃ¤ dataset uudestaan\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = N, shuffle=True)\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "images = real_batch[0] # TÃ¤hÃ¤n kaikki datasetin kuvat\n",
    "\n",
    "\n",
    "flat = torch.zeros(N,3*64*64)\n",
    "for i in range(N):\n",
    "    flat[i] = images[i].view(-1) # litistetÃ¤Ã¤n kuvat\n",
    "\n",
    "#print(flat.shape)\n",
    "#flatted = torch.cat([flatted,flat],0)\n",
    "#print(flatted.shape)\n",
    "dist1 = scp.cdist(flatted,flat,'cityblock') # Cityblock pitÃ¤isi olla L1 etÃ¤isyys\n",
    "\n",
    "#Y = scp.squareform(scp.pdist(flatted,'cityblock')) \n",
    "#dist = scp.pdist(flatted,'cityblock')[0:N]\n",
    "Ymin = np.min(dist1) # EtsitÃ¤Ã¤n pienin etÃ¤isyys (= samanlaisin kuva)\n",
    "print(Ymin)\n",
    "indx = np.where(dist1==Ymin) # EtsitÃ¤Ã¤n sen kuvan indeksi\n",
    "print(np.where(dist1==Ymin)) \n",
    "\n",
    "\n",
    "#Y = scp.cdist(flatted,flat)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(np.transpose(vutils.make_grid(fake_images.to(DEVICE)[Kuva_nmro],nrow=10, padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(DEVICE)[indx[1]],nrow=10, padding=2, normalize=True).cpu(),(1,2,0)))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "C_Cifar_new.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
